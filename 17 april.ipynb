{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffe0d1c-dbe4-4f9d-b2a7-a389612a9985",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240bcfb-6263-4acc-b313-be2b6de15841",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression is a machine learning technique that builds an ensemble of decision trees sequentially to predict continuous numerical values. Hereâ€™s a concise explanation:\n",
    "\n",
    "- **Ensemble Learning**: Gradient Boosting Regression combines multiple weak learners (typically shallow decision trees) sequentially to create a strong learner.\n",
    "\n",
    "- **Gradient Descent**: Unlike AdaBoost which focuses on adjusting instance weights, Gradient Boosting Regression minimizes a loss function (usually squared error loss for regression tasks) by gradient descent.\n",
    "\n",
    "- **Sequential Training**: It sequentially builds trees, where each subsequent tree fits the residuals (errors) of the previous tree.\n",
    "\n",
    "- **Model Complexity**: It can fit complex nonlinear relationships in data due to its ability to capture interactions and dependencies between features.\n",
    "\n",
    "- **Regularization**: Various regularization techniques are employed to prevent overfitting, such as controlling tree depth, learning rate, and early stopping.\n",
    "\n",
    "Gradient Boosting Regression algorithms such as Gradient Boosting Machines (GBM), XGBoost, LightGBM, and CatBoost are widely used due to their effectiveness in predicting continuous outcomes with high accuracy and robustness to noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b39da-b934-4dcd-9627-266fda566f12",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8490b6-b258-4e08-ae88-2276221654bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * (X - 5) ** 2 + np.random.randn(100, 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(X))\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560ef385-9beb-4b64-9456-0440ec61f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.residuals = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with mean of y\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.models.append(initial_prediction)\n",
    "        \n",
    "        # Fit each estimator sequentially\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - self.predict(X)\n",
    "            self.residuals.append(residuals)\n",
    "            \n",
    "            # Fit decision stump (weak learner) to residuals\n",
    "            stump = DecisionStump()\n",
    "            stump.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions by adding scaled stump prediction\n",
    "            self.models.append(stump)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.ones(len(X)) * self.models[0]\n",
    "        for model in self.models[1:]:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.split_feature = None\n",
    "        self.split_threshold = None\n",
    "        self.prediction = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Find best split\n",
    "        min_error = float('inf')\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] < threshold\n",
    "                right_indices = ~left_indices\n",
    "                error = np.sum((y[left_indices] - np.mean(y[left_indices])) ** 2) \\\n",
    "                      + np.sum((y[right_indices] - np.mean(y[right_indices])) ** 2)\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    self.split_feature = feature\n",
    "                    self.split_threshold = threshold\n",
    "                    self.prediction = np.mean(y[left_indices]), np.mean(y[right_indices])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.split_feature] < self.split_threshold, self.prediction[0], self.prediction[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7b46bc-f816-4ad0-95de-12bd174003a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 6.803210586991037\n",
      "R-squared: 0.9726079781335314\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X_train, y_train.flatten())\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance using metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31106a8b-4ccf-469b-b829-4e7027b99e8d",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth tooptimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f28d5a4-958a-4bb8-871f-a2230ea050da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 200}\n",
      "Best MSE: -15.145719917331515\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=0)\n",
    "\n",
    "# Define the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "# Define the scoring method (mean squared error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring=scorer, cv=5, verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on test data (if available)\n",
    "# X_test, y_test = ...\n",
    "# y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "# print(\"Test MSE:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc4ac7f-e10b-43a0-b1ac-422758c3da57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters: {'learning_rate': 0.08607100646678517, 'max_depth': 3, 'n_estimators': 246}\n",
      "Best MSE: -15.143677209201774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define the random search grid of parameters to sample from\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.05, 0.3),  # uniform distribution between 0.05 and 0.35\n",
    "    'n_estimators': randint(50, 300),     # discrete uniform distribution between 50 and 300\n",
    "    'max_depth': randint(1, 5)            # discrete uniform distribution between 1 and 5\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(estimator=gb_regressor, param_distributions=param_dist, n_iter=100, scoring=scorer, cv=5, verbose=1)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best MSE:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on test data (if available)\n",
    "# X_test, y_test = ...\n",
    "# y_pred = random_search.best_estimator_.predict(X_test)\n",
    "# print(\"Test MSE:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d33655-f487-462e-bf06-8e42557bec8a",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2174a-7ec9-4233-847c-23fb8874d4a5",
   "metadata": {},
   "source": [
    "## In the context of Gradient Boosting, a weak learner refers to a simple model that performs slightly better than random guessing on a classification or regression problem. Specifically:\n",
    "\n",
    "1. **Definition**: A weak learner is typically a model that has limited predictive power on its own, often with low complexity. For classification tasks, weak learners might be decision stumps (single-level decision trees), shallow decision trees, or linear models. For regression tasks, they could be decision stumps or simple linear regression models.\n",
    "\n",
    "2. **Role in Gradient Boosting**: In Gradient Boosting, weak learners are sequentially added to the ensemble. Each weak learner is trained on the residuals (errors) of the ensemble up to the current stage. By focusing on the mistakes made by the previous models, weak learners incrementally improve the overall prediction accuracy of the ensemble.\n",
    "\n",
    "3. **Characteristics**:\n",
    "   - **Low Complexity**: Weak learners are intentionally kept simple to facilitate the boosting process and prevent overfitting.\n",
    "   - **Slightly Better than Random**: While weak learners are not highly accurate individually, they perform better than random guessing, ensuring that each subsequent model contributes to reducing the overall prediction error.\n",
    "\n",
    "4. **Example**: In practice, weak learners can vary based on the specific implementation of Gradient Boosting. They might include decision trees with a maximum depth of one or two, linear models, or other models that are computationally inexpensive and easy to train.\n",
    "\n",
    "5. **Boosting Mechanism**: The concept of boosting revolves around combining multiple weak learners into a strong learner. Through iterative training, each weak learner is trained to correct the errors of the previous ones, gradually improving the ensemble's predictive performance.\n",
    "\n",
    "In summary, in Gradient Boosting, a weak learner is a basic model that is incorporated into the ensemble to collectively contribute to the final prediction. Despite their simplicity, when combined effectively through boosting, these weak learners can produce highly accurate predictions for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ceb29-23dd-435e-9bb8-ab2837fbd0f6",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487b62c-764a-4fa1-80d1-ec9651afc381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a659ec5c-ef03-4a76-8f50-56307c5a7889",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e98293-9d87-404f-b13c-0e9538e02401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
